{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import catboost as cb\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import ( StratifiedKFold, RepeatedStratifiedKFold,\n",
    "                                     GridSearchCV, KFold )\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, roc_auc_score \n",
    "\n",
    "from collections import Counter\n",
    "import optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore')\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-notebook')\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "sns.set(context=\"paper\", font=\"monospace\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stats(data):\n",
    "    stats = []\n",
    "    for col in data.columns:\n",
    "        stats.append((col, data[col].nunique(), data[col].isnull().sum() * 100 / data.shape[0],\n",
    "                    round((data[col].value_counts(normalize=True, dropna=False).values[0] * 100),2), \n",
    "                    data[col].dtype))\n",
    "        \n",
    "    stats_df = pd.DataFrame(stats, columns=['Feature', 'Unique_values', 'missing values %', \n",
    "                                            'biggest category %', 'type']) \n",
    "    return stats_df.sort_values('missing values %', ascending=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1568, 27) (672, 26)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('Train.csv')\n",
    "test_df = pd.read_csv('Test.csv')\n",
    "sub_df = pd.read_csv('SampleSubmission.csv') \n",
    "print(train_df.shape, test_df.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = train_df[train_df['Year_of_Birth'] < 1940].index \n",
    "# train_df.drop(a, axis=0, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.reset_index(drop=True, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Married     615\n",
       "Together    409\n",
       "Single      329\n",
       "Divorced    159\n",
       "Widow        50\n",
       "Alone         3\n",
       "YOLO          2\n",
       "Absurd        1\n",
       "Name: Marital_Status, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Marital_Status'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Married     249\n",
       "Together    171\n",
       "Single      151\n",
       "Divorced     73\n",
       "Widow        27\n",
       "Absurd        1\n",
       "Name: Marital_Status, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['Marital_Status'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['Absurd', 'YOLO']: \n",
    "    train_df.loc[train_df['Marital_Status'] == i, ['Marital_Status']] = 'others'\n",
    "    test_df.loc[test_df['Marital_Status'] == i, ['Marital_Status']] = 'others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = train_df['Disposable_Income'].median()\n",
    "train_df.fillna(value, inplace=True)\n",
    "test_df.fillna(value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['Disposable_Income'] = train_df['Disposable_Income'].interpolate(method=\"linear\")\n",
    "# test_df['Disposable_Income'] = test_df['Disposable_Income'].interpolate(method=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = train_df.shape[0]\n",
    "ntest = test_df.shape[0]\n",
    "\n",
    "data = pd.concat((train_df, test_df)).reset_index(drop=True)\n",
    "data['Date_Customer'] = pd.to_datetime(data['Date_Customer'])\n",
    "data['date_year'] = data['Date_Customer'].dt.year\n",
    "data['date_month'] = data['Date_Customer'].dt.month\n",
    "data['date_quat'] = data['Date_Customer'].dt.quarter \n",
    "\n",
    "data['rwp'] = data['Recency'] / (data['WebVisitsMonth'] + 1)\n",
    "data['cata'] = data['Amount_on_MeatProducts'] / (data['CatalogPurchases'] + 1)\n",
    "data['yearly_expenses'] = (data[['Amount_on_Fruits', 'Amount_on_MeatProducts', 'Amount_on_FishProducts',\n",
    "                                'Amount_on_SweetProducts', 'Amount_on_GoldProds']].sum(axis=1)) / 3\n",
    "data['spending_ratio'] = data['Disposable_Income'] / data['yearly_expenses'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_list = [f'Cmp{i}Accepted' for i in range(1,6)]\n",
    "# # drop_list = drop_list + ['Any_Complain', 'Disposable_Income','WebVisitsMonth','Year_of_Birth'] \n",
    "# drop_list = drop_list + ['Any_Complain'] \n",
    "# data.drop(drop_list, axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['Disposable_Income'] = data['Disposable_Income'].interpolate(method=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.get_dummies(data, columns=['Education_Level', 'Marital_Status'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in ['Marital_Status', 'Education_Level']:\n",
    "    data[feat] = pd.factorize(data[feat])[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['ID', 'Date_Customer'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1568, 32) (672, 32) (1568,)\n"
     ]
    }
   ],
   "source": [
    "train = data[:ntrain]\n",
    "test = data[ntrain:]\n",
    "target = train['Response'] \n",
    "print(train.shape, test.shape, target.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1568, 31) (672, 31) (1568,)\n"
     ]
    }
   ],
   "source": [
    "train.drop('Response', axis=1, inplace=True)\n",
    "test.drop('Response', axis=1, inplace=True)\n",
    "print(train.shape, test.shape, target.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = Counter(target)[0] / Counter(target)[1]\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=24) \n",
    "xgb_ = xgb.XGBClassifier(scale_pos_weight=weight, n_jobs=-1, max_depth=4, \n",
    "                        random_state=64, sub_sample=0.8, n_estimators=500,\n",
    "                        gamma=1, min_child_weight=10, colsample_bytree=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================Fold1=====================\n",
      "[07:46:46] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"sub_sample\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-auc:0.79511\n",
      "[243]\tvalidation_0-auc:0.89850\n",
      "\n",
      "Train score: 0.995613284592051\n",
      "\n",
      "Test score: 0.9069548872180451\n",
      "=====================Fold2=====================\n",
      "[07:46:47] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"sub_sample\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-auc:0.77976\n",
      "[229]\tvalidation_0-auc:0.88659\n",
      "\n",
      "Train score: 0.9900871120790231\n",
      "\n",
      "Test score: 0.9119674185463658\n",
      "=====================Fold3=====================\n",
      "[07:46:47] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"sub_sample\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-auc:0.80483\n",
      "[221]\tvalidation_0-auc:0.90132\n",
      "\n",
      "Train score: 0.9824103601151124\n",
      "\n",
      "Test score: 0.9122807017543859\n",
      "=====================Fold4=====================\n",
      "[07:46:47] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"sub_sample\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-auc:0.69549\n",
      "[250]\tvalidation_0-auc:0.84743\n",
      "[337]\tvalidation_0-auc:0.84743\n",
      "\n",
      "Train score: 0.9997608306758965\n",
      "\n",
      "Test score: 0.8552631578947368\n",
      "=====================Fold5=====================\n",
      "[07:46:47] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"sub_sample\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-auc:0.69298\n",
      "[250]\tvalidation_0-auc:0.90570\n",
      "\n",
      "Train score: 0.9960488449871664\n",
      "\n",
      "Test score: 0.9088345864661654\n",
      "=====================Fold6=====================\n",
      "[07:46:47] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"sub_sample\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-auc:0.83349\n",
      "[210]\tvalidation_0-auc:0.91980\n",
      "\n",
      "Train score: 0.9544819942443805\n",
      "\n",
      "Test score: 0.950187969924812\n",
      "=====================Fold7=====================\n",
      "[07:46:47] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"sub_sample\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-auc:0.78290\n",
      "[250]\tvalidation_0-auc:0.91510\n",
      "[253]\tvalidation_0-auc:0.91510\n",
      "\n",
      "Train score: 0.9964163490705453\n",
      "\n",
      "Test score: 0.9279448621553885\n",
      "=====================Fold8=====================\n",
      "[07:46:48] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"sub_sample\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-auc:0.82707\n",
      "[250]\tvalidation_0-auc:0.94925\n",
      "[268]\tvalidation_0-auc:0.94956\n",
      "\n",
      "Train score: 0.9985785953177257\n",
      "\n",
      "Test score: 0.9608395989974937\n",
      "=====================Fold9=====================\n",
      "[07:46:48] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"sub_sample\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-auc:0.81187\n",
      "[203]\tvalidation_0-auc:0.82396\n",
      "\n",
      "Train score: 0.926736885296668\n",
      "\n",
      "Test score: 0.8957175547564563\n",
      "=====================Fold10=====================\n",
      "[07:46:48] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"sub_sample\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-auc:0.75631\n",
      "[250]\tvalidation_0-auc:0.90183\n",
      "[288]\tvalidation_0-auc:0.90183\n",
      "\n",
      "Train score: 0.9997862874239865\n",
      "\n",
      "Test score: 0.9068813131313131\n",
      "Average Train score for 10 folds split: 0.9839920543802556\n",
      "Average Test score for 10 folds split: 0.9136872050845163\n",
      "Standard deviation for 10 folds split: 0.0275959065582046\n"
     ]
    }
   ],
   "source": [
    "mean_train, mean_test_val = [], []\n",
    "test_pred = np.zeros(test.shape[0])\n",
    "val_pred = np.zeros(train.shape[0])\n",
    "\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(train, target)):\n",
    "    X_train, X_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    print(f'=====================Fold{fold +1}=====================')\n",
    "    xgb_.fit(X_train, y_train, early_stopping_rounds=200, eval_metric='auc',\n",
    "            eval_set=[(X_test, y_test)], verbose=250)\n",
    "    train_predict = xgb_.predict_proba(X_train)[:,1]\n",
    "    test_predict = xgb_.predict_proba(X_test)[:,1]\n",
    "    val_pred[test_index] = test_predict\n",
    "    test_pred += xgb_.predict_proba(test)[:,1]\n",
    "\n",
    "    print(f'\\nTrain score: {roc_auc_score(y_train, train_predict)}')\n",
    "    print(f'\\nTest score: {roc_auc_score(y_test, test_predict)}')\n",
    "\n",
    "    mean_train.append(roc_auc_score(y_train, train_predict))\n",
    "    mean_test_val.append(roc_auc_score(y_test, test_predict))\n",
    "\n",
    "test_pred = test_pred / 10\n",
    "print(f'Average Train score for 10 folds split: {np.mean(mean_train)}') \n",
    "print(f'Average Test score for 10 folds split: {np.mean(mean_test_val)}')\n",
    "print(f'Standard deviation for 10 folds split: {np.std(mean_test_val)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    672.000000\n",
       "mean       0.237126\n",
       "std        0.253954\n",
       "min        0.019728\n",
       "25%        0.047505\n",
       "50%        0.114212\n",
       "75%        0.348421\n",
       "max        0.967661\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(test_pred).describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.4\n",
    "predictions = [1 if p > threshold else 0 for p in test_pred] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    523\n",
       "1    149\n",
       "Name: Response, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df['Response'] = predictions\n",
    "sub_df['Response'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv('csv/test-xgboost.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['Response'] = test_pred \n",
    "sub_df.to_csv('csv/to-weight-test-xgboost.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7782738095238095\n",
      "0.8475765306122449\n"
     ]
    }
   ],
   "source": [
    "print(523/672)\n",
    "print(1329/(1329 + 239)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=2020)\n",
    "lgb_ = lgb.LGBMClassifier(random_state=2021, scale_pos_weight=weight, n_estimators=1500,\n",
    "                        colsample_bytree=0.8, min_child_samples=10, subsample=0.7,\n",
    "                        subsample_freq=5, num_leaves=120, metric='auc', learning_rate=0.01,\n",
    "                        max_depth=7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================Fold1=====================\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\tvalid_0's auc: 0.878759\n",
      "Early stopping, best iteration is:\n",
      "[240]\tvalid_0's auc: 0.880326\n",
      "\n",
      "Train score: 0.9970230224780274\n",
      "\n",
      "Test score: 0.8803258145363408\n",
      "=====================Fold2=====================\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 0.932018\n",
      "\n",
      "Train score: 0.9193766041844911\n",
      "\n",
      "Test score: 0.9320175438596492\n",
      "=====================Fold3=====================\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\tvalid_0's auc: 0.854323\n",
      "[500]\tvalid_0's auc: 0.870927\n",
      "[750]\tvalid_0's auc: 0.880013\n",
      "[1000]\tvalid_0's auc: 0.886591\n",
      "Early stopping, best iteration is:\n",
      "[1006]\tvalid_0's auc: 0.887845\n",
      "\n",
      "Train score: 0.9998658318425762\n",
      "\n",
      "Test score: 0.8878446115288221\n",
      "=====================Fold4=====================\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\tvalid_0's auc: 0.890038\n",
      "[500]\tvalid_0's auc: 0.892857\n",
      "Early stopping, best iteration is:\n",
      "[325]\tvalid_0's auc: 0.89599\n",
      "\n",
      "Train score: 0.9987069300770008\n",
      "\n",
      "Test score: 0.8959899749373434\n",
      "=====================Fold5=====================\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\tvalid_0's auc: 0.93609\n",
      "[500]\tvalid_0's auc: 0.946742\n",
      "[750]\tvalid_0's auc: 0.946742\n",
      "Early stopping, best iteration is:\n",
      "[588]\tvalid_0's auc: 0.952068\n",
      "\n",
      "Train score: 0.9997083300925566\n",
      "\n",
      "Test score: 0.9520676691729324\n",
      "=====================Fold6=====================\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\tvalid_0's auc: 0.922932\n",
      "[500]\tvalid_0's auc: 0.930138\n",
      "[750]\tvalid_0's auc: 0.93515\n",
      "Early stopping, best iteration is:\n",
      "[658]\tvalid_0's auc: 0.936717\n",
      "\n",
      "Train score: 0.9997802753363927\n",
      "\n",
      "Test score: 0.93671679197995\n",
      "=====================Fold7=====================\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\tvalid_0's auc: 0.826441\n",
      "[500]\tvalid_0's auc: 0.835526\n",
      "[750]\tvalid_0's auc: 0.842105\n",
      "Early stopping, best iteration is:\n",
      "[749]\tvalid_0's auc: 0.842105\n",
      "\n",
      "Train score: 0.9999669440771565\n",
      "\n",
      "Test score: 0.8421052631578947\n",
      "=====================Fold8=====================\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\tvalid_0's auc: 0.91604\n",
      "Early stopping, best iteration is:\n",
      "[62]\tvalid_0's auc: 0.920113\n",
      "\n",
      "Train score: 0.9862137357081746\n",
      "\n",
      "Test score: 0.9201127819548872\n",
      "=====================Fold9=====================\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\tvalid_0's auc: 0.887872\n",
      "[500]\tvalid_0's auc: 0.887545\n",
      "Early stopping, best iteration is:\n",
      "[390]\tvalid_0's auc: 0.889833\n",
      "\n",
      "Train score: 0.9994832311408397\n",
      "\n",
      "Test score: 0.8898332788492972\n",
      "=====================Fold10=====================\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's auc: 0.944444\n",
      "\n",
      "Train score: 0.9806007266227583\n",
      "\n",
      "Test score: 0.9444444444444444\n",
      "Average Train score for 10 folds split: 0.9880725631559972\n",
      "Average Test score for 10 folds split: 0.9081458174421562\n",
      "Standard deviation for 10 folds split: 0.03287346565188375\n"
     ]
    }
   ],
   "source": [
    "mean_train, mean_test_val = [], []\n",
    "test_pred = np.zeros(test.shape[0])\n",
    "val_pred = np.zeros(train.shape[0])\n",
    "\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(train, target)):\n",
    "    X_train, X_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    print(f'=====================Fold{fold +1}=====================')\n",
    "    lgb_.fit(X_train, y_train, early_stopping_rounds=200, eval_metric='auc',\n",
    "            eval_set=[(X_test, y_test)], verbose=250)\n",
    "    train_predict = lgb_.predict_proba(X_train)[:,1]\n",
    "    test_predict = lgb_.predict_proba(X_test)[:,1]\n",
    "    val_pred[test_index] = test_predict\n",
    "    test_pred += lgb_.predict_proba(test)[:,1]\n",
    "\n",
    "    print(f'\\nTrain score: {roc_auc_score(y_train, train_predict)}')\n",
    "    print(f'\\nTest score: {roc_auc_score(y_test, test_predict)}')\n",
    "\n",
    "    mean_train.append(roc_auc_score(y_train, train_predict))\n",
    "    mean_test_val.append(roc_auc_score(y_test, test_predict))\n",
    "\n",
    "test_pred = test_pred / 10\n",
    "print(f'Average Train score for 10 folds split: {np.mean(mean_train)}') \n",
    "print(f'Average Test score for 10 folds split: {np.mean(mean_test_val)}')\n",
    "print(f'Standard deviation for 10 folds split: {np.std(mean_test_val)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    672.000000\n",
       "mean       0.179398\n",
       "std        0.186587\n",
       "min        0.035637\n",
       "25%        0.050163\n",
       "50%        0.088352\n",
       "75%        0.225735\n",
       "max        0.773399\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(test_pred).describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.27\n",
    "predictions = [1 if p > threshold else 0 for p in test_pred]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    527\n",
       "1    145\n",
       "Name: Response, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df['Response'] = predictions\n",
    "sub_df['Response'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv('csv/test-lgbm.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['Response'] = test_pred \n",
    "sub_df.to_csv('csv/to-weight-test-lgbm.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred = pd.read_csv('csv/to-weight-test-xgboost.csv')\n",
    "lgb_pred = pd.read_csv('csv/to-weight-test-lgbm.csv') \n",
    "\n",
    "pred = ((0.6 * xgb_pred['Response'] + 0.4 * lgb_pred['Response']) + (0.65 * xgb_pred['Response'] + 0.35 * lgb_pred['Response'])) / 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    672.000000\n",
       "mean       0.215478\n",
       "std        0.227295\n",
       "min        0.025694\n",
       "25%        0.048987\n",
       "50%        0.107061\n",
       "75%        0.307521\n",
       "max        0.892554\n",
       "Name: Response, dtype: float64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(pred).describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.27\n",
    "predictions = [1 if p > threshold else 0 for p in pred]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    472\n",
       "1    200\n",
       "Name: Response, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df['Response'] = predictions\n",
    "sub_df['Response'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv('csv/test-weighted.csv', index=False)  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee89bb07d3fa5f9c0a8bd127b3056783b076f56308f3ae1a55a1b4c1c97796ac"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
